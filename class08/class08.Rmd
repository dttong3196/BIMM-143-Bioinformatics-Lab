---
title: "class08 Machine Learning 1"
author: "Duy Tong"
date: "10/24/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K-means Clustering

```{r}
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```
Use the kmeans() function setting k to 2 and nstart=20

#Use kmean()
```{r}
k <- kmeans(x, centers = 2, nstart = 20)
```

#Inspect / Print the Result
```{r}
k
```
# Cluster Means: the center -> -3 & 3
# Cluster Vector: 
#Available Components:

#Q. How many points are in each cluster?
  30 points each.
  
#Q. What ‘component’ of your result object details
- cluster size?
- cluster assignment/membership?
- cluster center?
#Cluster Size?
```{r}
k$size
```

# Membership / Assignment
```{r}
k$cluster
```

#Cluster Center?
```{r}
k$centers
```

#Plot x colored by the kmeans cluster assignment and add cluster centers as blue points
```{r}
plot(x, col=k$cluster)
points(k$centers, col = "blue", pch = 15)
```

## Hierarchinal Clustering in R
the `hclust()` function requires a distance matrix as input. You can get this from the `dist()` function

```{r}
# First we need to calculate point (dis)similarity
# as the Euclidean distance between observations
dist_matrix <- dist(x)
# The hclust() function returns a hierarchical
# clustering model
hc <- hclust(d = dist_matrix)
# the print method is not so useful here
hc

```

```{r}
plot(hc)
```
# the first 30 points in this branch & next 30 points in another branch.

#Interpreting Results
```{r}
# Draws a dendrogram
plot(hc)
abline (h=6, col= "red")

```

# Cut Tree
```{r}
# Draws a dendrogram
plot(hc)
abline (h=6, col= "red")
cutree(hc, h = 6)
```

```{r}
# Draws a dendrogram
plot(hc)
abline (h=6, col= "red")
grps <- cutree(hc, h = 6)
```

```{r}
cutree(hc, k = 2)
```

# Step 1. Generate Some Example Data for Clustering
```{r}
x <- rbind(
matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
```

# Step 2. Plot the Data Without Clustering
```{r}
plot(x)
```

# Step 3. Generate Colors for Known Clusters--to compare to hclust results
```{r}
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```
# Q. Use the dist(), hclust(), plot() and cutree() functions to return 2 and 3 clusters
```{r}
#clustering
hc <- hclust(dist(x))

#draw tree
plot (hc)
abline (h =2, col = "red")

#Cut tree into clusters / groups
grps <- cutree(hc, k = 3)
grps
```

Plot the data colored by their hclust results 
```{r}
plot(x, col = grps)
```

#How many Points in Each Cluster
```{r}
table(grps)
```

#Cross-Tabulate 
```{r}
table(grps, col)
```

```{r}
mydata <- read.csv("https://tinyurl.com/expression-CSV",
row.names=1)
head(mydata)
```

#Number of row
```{r}
#Dimension of Object
dim(mydata)
#Number of Genes
nrow(mydata)
```

#R in PCA with the **prcomp()** function.
```{r}
pca <- prcomp(t(mydata), scale=TRUE)
# See what is returned by the prcomp() function
attributes(pca)
```

## A basic PC1 vs PC2 2-D Plot
```{r}
plot(pca$x[ , 1], pca$x [, 2])
```

# Variance Captured per PC
```{r}
pca.var <- pca$sdev^2
```

## Precent variance is often more informative to look at
```{r}
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)

pca.var.per
```
# Make Barplot of PCA
```{r}
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
barplot(pca.var.per, main="Scree Plot",
xlab="Principal Component", ylab="Percent Variation")
pca.var.per
```

#Color the Points
```{r}
plot(pca$x[ ,1], pca$x[ ,2], col = c("red", "red", "red", "red", "red", "blue", "blue", "blue", "blue", "blue"))
colvec <- colnames(mydata)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"
plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
xlab=paste0("PC1 (", pca.var.per[1], "%)"),
ylab=paste0("PC2 (", pca.var.per[2], "%)"))
#Add Labels
plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
xlab=paste0("PC1 (", pca.var.per[1], "%)"),
ylab=paste0("PC2 (", pca.var.per[2], "%)"))
## Click to identify which sample is which
identify(pca$x[,1], pca$x[,2], labels=colnames(mydata))
## Press ESC to exit…
```

# Lecture 8 Lab: PCA on Food
```{r}
x <- read.csv("UK_foods (1).csv")
head(x)
```

# Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this question
  
```{r}
dim(x)
nrow(x)
ncol(x)
#There are 17 rows and 5 columns.
```

## Complete the following code to find out how many rows and columns are in x?
```{r}
dim(x)
```

# Checking Your Data
## Preview the First 6 Rows
```{r}
#Could Also Specify **row.names()**
x <- read.csv("UK_foods (1).csv", row.names = 1)
head(x)
```

# Note How the Minus Indexing Works
```{r}
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```

# Check the Dimensions Again
```{r}
dim(x)
```

#Alternative Approach
```{r}
x <- read.csv("UK_foods (1).csv", row.names = 1)
head(x)
```

#Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?
  Giving an additional argument **row.names() gives a better image of the data because it shows most detailed.
  
#Spotting Major Differences and Trends (Listen at 1 Hr 3 minutes Part 2)
barplot(barplot(as.matrix(x), beside=T, col=rainbow(nrow(x))))
```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```
#Q3: Changing what optional argument in the above barplot() function results in the following plot?
```{r}
barplot(as.matrix(x), beside=FALSE, col=rainbow(nrow(x)))
```

#Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?
```{r}
pairs(x, col=rainbow(10), pch=16)
```

#Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?
  N. Ireland for PC 1 range is further away from other countries; whereas, England, Scotland, Wales are closely related to each other. This PCA graph is effective in a sense that we're comparing only 4 PCAs; if we happen to compare more set of data then it would be complicated to find the signficant.

```{r}
pca <-prcomp( t(x))
summary(pca)
```
# prooprtion of variance shows us true variance. How PC1 is the biggest!

#Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.
```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```

# Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.
```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col = c( "red", "yellow", "blue", "green"))
```

# More PCA Information
```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

# or the second row here...
```{r}
z <- summary(pca)
z$importance
```

#barplot
```{r}
barplot(v, xlab = "Principal Component", ylab = "Percent Variation")
```

# Digging Deeper (Variable Loadings)
## Lets focus on PC1 as it accounts for > 90% of variance 
```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```
#Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?
  The two food groups that are feature prominantely are fresh potatoes in PC1 -> which pushes N. Ireland out toward the positive (this is shown beacause fresh_potatoes is most highest positive), wheraas, the negative valeus (fresh_fruit, Alcoholic drinks and other veg) fresh_fruit = Wales, Alcoholic drink = england, other veg = scotland. 
  In PC2 -> fresh potatoes shows Wales, but whereas, solf_drinks = shows  for scottland.
```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2)
```
  
# Biplots
```{r}
biplot(pca)
```

# PCA of RNA-seq Data
```{r}
rna.data <- read.csv("expression.csv", row.names=1)
head(rna.data)
```

#Q10: How many genes and samples are in this data set?
```{r}
dim(rna.data)
# 100 genes and 10 samples
```
#Make barplot
```{r}
## Again we have to take the transpose of our data 
pca <- prcomp(t(rna.data), scale=TRUE)
 
## Simple un ploished plot of pc1 and pc2
plot(pca$x[,1], pca$x[,2])
```

```{r}
## Variance captured per PC 
pca.var <- pca$sdev^2

## Precent variance is often more informative to look at 
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```

```{r}
barplot(pca.var.per, main="Scree Plot", 
        xlab="Principal Component", ylab="Percent Variation")
```

```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(rna.data)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
     xlab=paste0("PC1 (", pca.var.per[1], "%)"),
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))

text(pca$x[,1], pca$x[,2], labels = colnames(rna.data), pos=c(rep(4,5), rep(2,5)))
```

```{r}
## Another way to color by sample type
## Extract the first 2 characters of the sample name
sample.type <- substr(colnames(rna.data),1,2)
sample.type
```

```{r}
## now use this as a factor input to color our plot
plot(pca$x[,1], pca$x[,2], col=as.factor(sample.type), pch=16)
```

